{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from envirocar import TrackAPI, DownloadClient, BboxSelector, ECConfig\n",
    "import plotly.express as px\n",
    "\n",
    "# pandas show all columns of table instead of restricted\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# create an initial but optional config and an api client\n",
    "config = ECConfig()\n",
    "track_api = TrackAPI(api_client=DownloadClient(config=config))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------- data preprocessing functions ----------------\n",
    "\n",
    "\n",
    "# Percentages correction\n",
    "def percentages_correction(df):\n",
    "    '''\n",
    "        Aim: \n",
    "            Set faulty percentages (percentages below 0 and above 100) to nan\n",
    "        \n",
    "        Input: \n",
    "            Geodataframa\n",
    "        \n",
    "        Output: \n",
    "            Dataframe with corrected percentages\n",
    "    '''\n",
    "    df[\"faulty_percentages\"] = np.nan\n",
    "    units = df.filter(like='.unit').columns\n",
    "    values = df.filter(like='.value').columns\n",
    "    for col in df:\n",
    "        if col in units:\n",
    "            if df[col].iloc[0]== \"%\":\n",
    "                name = col.split(\".\")[0] + '.value'\n",
    "                if name in values:\n",
    "                    if any(df[name] < 0) or any(df[name] > 100):\n",
    "                        nanBefore = df[name].isna().sum(axis=0)\n",
    "                        df[name][df[name] < 0] = np.nan\n",
    "                        df[name][df[name] > 100] = np.nan\n",
    "                        nanAfter = df[name].isna().sum(axis=0)\n",
    "                        corrected = nanAfter - nanBefore\n",
    "                        print( 'Percentages ok : ', df[name].name, ' Count corrected:', corrected)        \n",
    "                    else:\n",
    "                        print('Percentages ok : ', df[name].name )\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def flag_faulty_percentages(df):\n",
    "    '''\n",
    "        Aim: \n",
    "            Inspect if there are faulty percentages (percentages below 0 and above 100)\n",
    "        \n",
    "        Input: \n",
    "            Geodataframa\n",
    "        \n",
    "        Output: \n",
    "            Geodataframe with added column which contains when percentages are faulty\n",
    "    '''\n",
    "    df[\"faulty_percentages\"] = 0\n",
    "    units = df_tracks.filter(like='.unit').columns\n",
    "    # values = df.filter(like='.value').columns\n",
    "\n",
    "    listNames =[]\n",
    "    for col in units:\n",
    "        if df_tracks[col].iloc[0]== '%':\n",
    "            name = col.split(\".\")[0] + '.value'\n",
    "            listNames.append(name)\n",
    "        \n",
    "    for variable in listNames:\n",
    "        df_tracks.loc[df_tracks[variable] < 0, 'faulty_percentages'] = 1\n",
    "        df_tracks.loc[df_tracks[variable] > 100, 'faulty_percentages'] = 1\n",
    "\n",
    "    faultyPercentages = (df_tracks['faulty_percentages'].values == 1).sum()\n",
    "    print('Flagged faulty percentages: ', faultyPercentages)\n",
    "    # df_tracks.loc[df_tracks['faulty_percentages'] == 1] \n",
    "    return df\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "def flag_implausible_negative_values(df,listOfVariableNames):\n",
    "    '''\n",
    "        Aim: Inspect if there are unexpected negative values\n",
    "        \n",
    "        Input: Geodataframa\n",
    "        \n",
    "        Output: Geodataframe with added column which contains 1 when values are negative\n",
    "    '''   \n",
    "    df[\"implausible_neg_value\"] = 0\n",
    "    for variable in listOfVariableNames:\n",
    "        df.loc[df[variable] < 0, 'implausible_neg_value'] = 1\n",
    "    implausibleNegativeValues = (df['implausible_neg_value'].values == 1).sum()\n",
    "    print('Flagged implausible negative values: ', implausibleNegativeValues)\n",
    "    return df\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def flag_outlier_in_sample(df, listOfVariableNames, dropOutlierColumn=False):\n",
    "    '''\n",
    "        Aim: Find outlier with regard to the sample's distribution \n",
    "        \n",
    "        Input: Geodataframa\n",
    "        \n",
    "        Output: Geodataframe with added column which values are '1' \n",
    "                when a certain value of a variable in the list is considered to be an outlier regarding the samples's distribution\n",
    "    '''\n",
    "    df['outlier_in_sample'] = 0\n",
    "    for variable in listOfVariableNames:\n",
    "        variableName='outlier_in_sample_'+ variable\n",
    "        df[variableName] = 0\n",
    "        Q1 = df[variable].quantile(0.25)\n",
    "        Q3 = df[variable].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        low_lim = Q1 - 1.5 * IQR \n",
    "        up_lim = Q3 + 1.5 * IQR  \n",
    "        df.loc[df[variable] < low_lim, variableName] = 1\n",
    "        df.loc[df[variable] > up_lim, variableName] = 1\n",
    "        df.loc[df[variable] < low_lim, 'outlier_in_sample'] = 1\n",
    "        df.loc[df[variable] > up_lim, 'outlier_in_sample'] = 1\n",
    "        if dropOutlierColumn == True:\n",
    "            df.drop([variableName], axis=1, inplace=True)\n",
    "    outlier = (df['outlier_in_sample'].values == 1).sum()\n",
    "    print('Flagged outlier in sample: ', outlier)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flag_outlier_in_track(df, listOfVariableNames, dropLimits=True, dropOutlierColumn=False):\n",
    "    '''\n",
    "        Aim: Inspect outlier with regard to the tracks' distribution \n",
    "        \n",
    "        Input: Geodataframa\n",
    "        \n",
    "        Output: Geodataframe with added column which values are '1'\n",
    "                when a certain value of a variable in the list is considered to be an outlier regarding the samples's distribution\n",
    "    '''\n",
    "    \n",
    "    def low_limit(x):\n",
    "            q1 = x.quantile(0.25)\n",
    "            q3 = x.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_limit = q1 - 1.5 * iqr\n",
    "            return lower_limit\n",
    "\n",
    "    def upper_limit(x):\n",
    "            q1 = x.quantile(0.25)\n",
    "            q3 = x.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            upper_limit = q3 + 1.5 * iqr\n",
    "            return upper_limit\n",
    "        \n",
    "    \n",
    "    #def calculate_limits(df, listOfVariableNames): \n",
    "     #   for variable in listOfVariableNames:\n",
    "      #      lowName = 'track_lowerLimit_' + variable\n",
    "       #     upName = 'track_upperLimit_' + variable\n",
    "        #    df_1 = df.groupby(['track.id'])\n",
    "         #   df[lowName] = df_1[variable].transform(low_limit)#.rename(columns={'low_limit': lowName, 'upper_limit': upName})\n",
    "          #  df[upName] = df_1[variable].transform(upper_limit)\n",
    "    \n",
    "            ##df_1 = df.groupby(['track.id']).agg({variable: [low_limit, upper_limit]}).rename(columns={'low_limit': lowName, 'upper_limit': upName})\n",
    "            ##print(df_1.index)\n",
    "            ##df_1.columns = df_1.columns.droplevel(0)\n",
    "            ##df = pd.merge(df, df_1, how='inner', on = 'track.id')     \n",
    "        #return df\n",
    "    \n",
    "    df['outlier_in_track_all'] = 0\n",
    "    for variable in listOfVariableNames:\n",
    "            lowName = 'track_lowerLimit_' + variable\n",
    "            upName = 'track_upperLimit_' + variable\n",
    "            df_1 = df.groupby(['track.id'])\n",
    "            df[lowName] = df_1[variable].transform(low_limit)#.rename(columns={'low_limit': lowName, 'upper_limit': upName})\n",
    "            df[upName] = df_1[variable].transform(upper_limit)\n",
    "            df.loc[df[upName] < df[variable], \"outlier_in_track_all\"] = 1 \n",
    "            df.loc[df[lowName] > df[variable], \"outlier_in_track_all\"] = 1 \n",
    "            variableName='outlier_in_track_'+ variable\n",
    "            df[variableName] = 0\n",
    "            df.loc[df[upName] < df[variable], variableName] = 1 \n",
    "            df.loc[df[lowName] > df[variable], variableName] = 1\n",
    "            print(variableName, (df[variableName].values == 1).sum())\n",
    "            \n",
    "            if dropLimits == True:\n",
    "                df.drop([upName, lowName], axis=1, inplace=True)\n",
    "            \n",
    "            if dropOutlierColumn == True:\n",
    "                df.drop([variableName], axis=1, inplace=True)\n",
    "    \n",
    "    outlier = (df['outlier_in_track_all'].values == 1).sum()\n",
    "    print('Rows which contain outliers in tracks  (there may be multiple outlier in a single row) : ',outlier)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    #df_new = calculate_limits(df, listOfVariableNames)\n",
    "    #return df_new\n",
    "    #df_new['outlier_in_track_all'] = 0\n",
    "    #print(df_new)\n",
    "    #for variable in listOfVariableNames:\n",
    "     #   upName = 'track_upperLimit_' + variable\n",
    "      #  lowName = 'track_lowerLimit_' + variable\n",
    "        \n",
    "       # df_new.loc[df_new[upName] < df_new[variable], \"outlier_in_track_all\"] = 1 \n",
    "        #df_new.loc[df_new[lowName] > df_new[variable], \"outlier_in_track_all\"] = 1 \n",
    "        \n",
    "        #variableName='outlier_in_track_'+ variable\n",
    "        #df_new[variableName] = 0\n",
    "        #df_new.loc[df_new[upName] < df_new[variable], variableName] = 1 \n",
    "        #df_new.loc[df_new[lowName] > df_new[variable], variableName] = 1\n",
    "        #print(variableName, (df_new[variableName].values == 1).sum())\n",
    "        \n",
    "        #if dropLimits == True:\n",
    "         #   df_new.drop([upName, lowName], axis=1, inplace=True)\n",
    "            \n",
    "        #if dropOutlierColumn == True:\n",
    "         #   df_new.drop([variableName], axis=1, inplace=True)\n",
    "    \n",
    "    #outlier = (df_new['outlier_in_track_all'].values == 1).sum()\n",
    "    #print('Rows which contain outliers in tracks  (there may be multiple outlier in a single row) : ',outlier)\n",
    "    #return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlier_in_track_Speed.value 3\n",
      "outlier_in_track_CO2.value 19\n",
      "outlier_in_track_Rpm.value 6\n",
      "outlier_in_track_Consumption (GPS-based).value 14\n",
      "outlier_in_track_Consumption.value 19\n",
      "outlier_in_track_CO2 Emission (GPS-based).value 14\n",
      "Rows which contain outliers in tracks  (there may be multiple outlier in a single row) :  34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9,\n",
       "            ...\n",
       "             8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
       "           dtype='int64', length=245)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox = BboxSelector([\n",
    "    7.554130554199218, # min_x\n",
    "    51.95590322041212, # min_y\n",
    "    7.590351104736328, # max_x\n",
    "    51.97874790276371  # max_y\n",
    "])\n",
    "\n",
    "# issue a query\n",
    "df_tracks = track_api.get_tracks(bbox=bbox, num_results=10) \n",
    "df_tracks.index\n",
    "df_tracks['index'] = df_tracks.index\n",
    "\n",
    "\n",
    "listNonNegative=['Speed.value', 'CO2.value','Rpm.value',\n",
    "                 'Consumption (GPS-based).value',\n",
    "                 'Consumption.value',\n",
    "                 'CO2 Emission (GPS-based).value']\n",
    "\n",
    "\n",
    "#df_faultyPerc = flag_faulty_percentages(df_tracks)\n",
    "#df_negValues =  flag_implausible_negative_values(df_faultyPerc,listNonNegative)\n",
    "#df_outlier = flag_outlier_in_sample(df_negValues, listNonNegative,dropOutlierColumn=True)\n",
    "df_outlier= flag_outlier_in_track(df_tracks, listNonNegative, dropLimits=False)\n",
    "df_outlier.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_outlier[df_outlier['outlier_in_track_all'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9,\n",
       "            ...\n",
       "             8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
       "           dtype='int64', length=245)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_tracks.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracks['outlier_in_track'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracks.loc[df_tracks['implausible_neg_value']==1,['col2','col3']] = np.nan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:envirocar_quality] *",
   "language": "python",
   "name": "conda-env-envirocar_quality-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
